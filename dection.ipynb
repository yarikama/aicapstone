{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c112a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c980d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deadlift 0, Squat 1, BenchPress 2\n",
    "labels = ['deadlift', 'squat', 'benchpress']\n",
    "LABEL_DICT = {labels[i]: i for i in range(len(labels))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273a3e5",
   "metadata": {},
   "source": [
    "# 將每個影片處理成資料集 (角度、人體關鍵點)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc04be",
   "metadata": {},
   "source": [
    "## 三維角度計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    x1, y1, z1 = landmark1.x, landmark1.y, landmark1.z\n",
    "    x2, y2, z2 = landmark2.x, landmark2.y, landmark2.z\n",
    "    x3, y3, z3 = landmark3.x, landmark3.y, landmark3.z\n",
    "\n",
    "    # 計算兩個邊向量\n",
    "    vec1 = [x1 - x2, y1 - y2, z1 - z2]\n",
    "    vec2 = [x3 - x2, y3 - y2, z3 - z2]\n",
    "\n",
    "    # 計算向量長度\n",
    "    length1 = math.sqrt(vec1[0]**2 + vec1[1]**2 + vec1[2]**2)\n",
    "    length2 = math.sqrt(vec2[0]**2 + vec2[1]**2 + vec2[2]**2)\n",
    "\n",
    "    # 計算點乘\n",
    "    dotProduct = vec1[0]*vec2[0] + vec1[1]*vec2[1] + vec1[2]*vec2[2]\n",
    "\n",
    "    # 計算夾角(弧度制)\n",
    "    angleRad = math.acos(dotProduct / (length1 * length2))\n",
    "\n",
    "    # 將弧度制轉換為角度制\n",
    "    angleDegree = math.degrees(angleRad)\n",
    "\n",
    "    if angleDegree < 0:\n",
    "        angleDegree += 360\n",
    "\n",
    "    return angleDegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599595a0",
   "metadata": {},
   "source": [
    "## 人體大關節角度（非監督模型用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostureAngles(landmarks):    \n",
    "\n",
    "    left_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    \n",
    "    right_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value])   \n",
    "    \n",
    "    left_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_HIP.value])\n",
    "\n",
    "    right_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value])\n",
    "\n",
    "    left_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "\n",
    "    right_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    \n",
    "    angles = np.array([left_elbow_angle, right_elbow_angle, left_shoulder_angle, right_shoulder_angle, left_knee_angle, right_knee_angle])    \n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dea2cc",
   "metadata": {},
   "source": [
    "## 對每一個影片拆分成frame 100的片段，在進行姿勢辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructDatasetFromVideo(video, pose, label, dataPath, videoIndex, sliceLength=100):\n",
    "    \n",
    "    # 讀取影片\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    # 取得影片的總幀數，加快處理速度\n",
    "    totalFrameNumber = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # 32 個 （x, y, z）和 1 個 label\n",
    "    print(f\"video: {video} label: {label} total frame number: {totalFrameNumber}\")\n",
    "\n",
    "    # 將影片分割成多個片段\n",
    "    sliceVideoFrame = list(range(sliceLength, totalFrameNumber, sliceLength))\n",
    "    \n",
    "    # 對片段中的每張frame進行處理\n",
    "    for sliceIndex in range(len(sliceVideoFrame)):\n",
    "         \n",
    "        # 一個片段有 sliceLength 張frame\n",
    "        video_data_points = np.zeros((sliceLength, 33 * 4))\n",
    "        video_data_angles = np.zeros((sliceLength, 6))\n",
    "        \n",
    "        # 對每個frame進行處理\n",
    "        for frameNumber in tqdm(range(sliceLength), desc=f\"video {videoIndex} slice {sliceIndex}\"):\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # 跳過某些前面暖身的片段\n",
    "            if sliceIndex == 0:\n",
    "                continue\n",
    "            \n",
    "            frameResults = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # 紀錄每個frame的pose\n",
    "            if frameResults.pose_landmarks:\n",
    "                video_data_angles[frameNumber] = getPostureAngles(frameResults.pose_landmarks.landmark)\n",
    "                for i in range(33):\n",
    "                    poseValue = frameResults.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value]\n",
    "                    video_data_points[frameNumber][i * 3] = poseValue.x\n",
    "                    video_data_points[frameNumber][i * 3 + 1] = poseValue.y\n",
    "                    video_data_points[frameNumber][i * 3 + 2] = poseValue.z\n",
    "                    video_data_points[frameNumber][i * 3 + 3] = poseValue.visibility\n",
    "            \n",
    "        # 跳過某些前面暖身的片段\n",
    "        if sliceIndex == 0:\n",
    "            continue\n",
    "        # 將每個片段的資料存入檔案\n",
    "        else:\n",
    "            np.save(f\"{dataPath}/{label}/points/{videoIndex}_{sliceIndex}_points.npy\", video_data_points)\n",
    "            np.save(f\"{dataPath}/{label}/angles/{videoIndex}_{sliceIndex}_angles.npy\", video_data_angles)\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2616b5",
   "metadata": {},
   "source": [
    "# 創立資料夾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceL = [70, 100, 120, 150]\n",
    "\n",
    "for i in sliceL:\n",
    "    dataPath = \"data\" + str(i)\n",
    "\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.makedirs(dataPath)\n",
    "        \n",
    "    for label in labels:\n",
    "        if not os.path.exists(f\"{dataPath}/{label}\"):\n",
    "            os.makedirs(f\"{dataPath}/{label}\")\n",
    "            os.makedirs(f\"{dataPath}/{label}/points\")\n",
    "            os.makedirs(f\"{dataPath}/{label}/angles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c04b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceL = [70, 100, 120, 150]\n",
    "\n",
    "for i in sliceL:\n",
    "    dataPath = \"data\" + str(i)\n",
    "    print(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2fb0b",
   "metadata": {},
   "source": [
    "# 處理每個影片並歸類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9809bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定pose model，影片、鬱值等\n",
    "pose = mp_pose.Pose(static_image_mode = False, min_detection_confidence = 0.4, min_tracking_confidence = 0.4, model_complexity = 2)\n",
    "\n",
    "# 對每個label的資料夾內的影片進行處理\n",
    "for label in labels:\n",
    "    directoryPath = f'{label}_raw'\n",
    "    video_paths = [os.path.join(directoryPath, f) for f in os.listdir(directoryPath) if f.endswith('.mp4')]\n",
    "    \n",
    "    for video in video_paths:\n",
    "        for i in sliceL:\n",
    "            dataPath = \"data\" + str(i)\n",
    "            constructDatasetFromVideo(video, pose, label, dataPath, video_paths.index(video), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f5ccb",
   "metadata": {},
   "source": [
    "# 處理資料成數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076602d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    70: {\"points\": [], \"labels\": [], \"angles\": []},\n",
    "    100: {\"points\": [], \"labels\": [], \"angles\": []},\n",
    "    120: {\"points\": [], \"labels\": [], \"angles\": []},\n",
    "    150: {\"points\": [], \"labels\": [], \"angles\": []}\n",
    "}\n",
    "\n",
    "for label in labels:\n",
    "    for slice_length in sliceL:\n",
    "        data_path = f\"data{slice_length}\"\n",
    "        \n",
    "        for data_type in [\"points\", \"angles\"]:\n",
    "            files = os.listdir(f\"{data_path}/{label}/{data_type}\")\n",
    "            files.sort()\n",
    "            \n",
    "            for file in files:\n",
    "                data = np.load(f\"{data_path}/{label}/{data_type}/{file}\")\n",
    "                \n",
    "                if data_type == \"points\":\n",
    "                    data_dict[slice_length][\"points\"].append(data)\n",
    "                    data_dict[slice_length][\"labels\"].append(LABEL_DICT[label])\n",
    "                else:\n",
    "                    data_dict[slice_length][\"angles\"].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_data = np.array(data_dict[150][\"points\"])\n",
    "angles_data = np.array(data_dict[150][\"angles\"])\n",
    "labels_data = to_categorical(data_dict[150][\"labels\"]).astype(int)\n",
    "print(points_data.shape, angles_data.shape, labels_data.shape)\n",
    "\n",
    "print(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_train, points_test,  angles_train, angles_test,  labels_train, labels_test  = train_test_split(points_data, angles_data, labels_data, test_size=0.10)\n",
    "points_train, points_valid, angles_train, angles_valid, labels_train, labels_valid = train_test_split(points_train, angles_train, labels_train, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19671aff",
   "metadata": {},
   "source": [
    "# LSTM 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Logs'):\n",
    "    os.makedirs('Logs')\n",
    "\n",
    "print(np.shape(points_train[0]))\n",
    "print(np.shape(angles_train[0]))\n",
    "print(np.shape(labels_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53010761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, activation='relu', input_shape=(np.shape(points_train[0]))))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(256, return_sequences=True, activation='relu', input_shape=(np.shape(points_train[0]))))\n",
    "# model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(16, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置 TensorBoard \n",
    "tb_callback = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456203c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型並進行驗證集驗證\n",
    "batch_size = 8\n",
    "epochs = 200\n",
    "\n",
    "model.fit(points_train, labels_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(points_valid, labels_valid),\n",
    "          callbacks=[tb_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f60023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('broad150.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('best_80.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e297ab",
   "metadata": {},
   "source": [
    "# 預測 & 評估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd667324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# 在測試集上評估模型\n",
    "loss, accuracy = model.evaluate(points_test, labels_test, verbose=0)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 獲取測試集的預測結果\n",
    "y_pred = model.predict(points_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(labels_test, axis=1)\n",
    "\n",
    "# 創建一個包含5個子圖的圖形\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "# 計算混淆矩陣並繪製圖形\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Labels')\n",
    "axes[0].set_ylabel('True Labels')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# 計算精確率、召回率、F1分數並繪製圖形\n",
    "report = classification_report(y_true_labels, y_pred_labels, output_dict=True)\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['precision'] for k in list(report.keys())[:-3]], ax=axes[1])\n",
    "axes[1].set_xlabel('Classes')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision')\n",
    "\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['recall'] for k in list(report.keys())[:-3]], ax=axes[2])\n",
    "axes[2].set_xlabel('Classes')\n",
    "axes[2].set_ylabel('Recall')\n",
    "axes[2].set_title('Recall')\n",
    "\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['f1-score'] for k in list(report.keys())[:-3]], ax=axes[3])\n",
    "axes[3].set_xlabel('Classes')\n",
    "axes[3].set_ylabel('F1 Score')\n",
    "axes[3].set_title('F1 Score')\n",
    "\n",
    "# 計算ROC曲線並繪製圖形\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(len(np.unique(y_true_labels))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(labels_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "for i in range(len(np.unique(y_true_labels))):\n",
    "    axes[4].plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "axes[4].plot([0, 1], [0, 1], 'k--')\n",
    "axes[4].set_xlabel('False Positive Rate')\n",
    "axes[4].set_ylabel('True Positive Rate')\n",
    "axes[4].set_title('ROC Curve')\n",
    "axes[4].legend(loc='lower right')\n",
    "\n",
    "# 調整子圖之間的間距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 顯示圖形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62795ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_lengths = [70, 100, 120, 150]\n",
    "test_losses = [0.4708, 1.1183, 1.1026, 50372.9922]\n",
    "test_accuracies = [0.8052, 0.1961, 0.4048, 0.4062]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "color1 = 'tab:red'\n",
    "ax1.set_xlabel('Frame Length')\n",
    "ax1.set_ylabel('Test Loss', color=color1)\n",
    "ax1.plot(frame_lengths, test_losses, color=color1, marker='o', linestyle='-', linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color2 = 'tab:blue'\n",
    "ax2.set_ylabel('Test Accuracy', color=color2)\n",
    "ax2.plot(frame_lengths, test_accuracies, color=color2, marker='o', linestyle='-', linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('Test Loss and Test Accuracy vs Frame Length')\n",
    "plt.xticks(frame_lengths)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab0bc3",
   "metadata": {},
   "source": [
    "# 建立隨機森林模型（使用關鍵點的角度、不使用關鍵點本身）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_data = np.array(data_dict[70][\"angles\"])\n",
    "labels_data = np.array(data_dict[70][\"labels\"]).astype(int)\n",
    "angles_data_2d = angles_data.reshape(angles_data.shape[0], -1)\n",
    "\n",
    "print(angles_data_2d.shape, labels_data.shape)\n",
    "\n",
    "angles_train, angles_test,  labels_train, labels_test  =  train_test_split(angles_data_2d, labels_data,  test_size=0.10)\n",
    "angles_train, angles_valid, labels_train, labels_valid =  train_test_split(angles_train,   labels_train, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建隨機森林分類器\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# 訓練模型\n",
    "rf_model.fit(angles_train, labels_train)\n",
    "\n",
    "# 在驗證集上進行預測\n",
    "y_pred_valid = rf_model.predict(angles_valid)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred_test = rf_model.predict(angles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938166bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建一個包含5個子圖的圖形\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "accuracy_test = accuracy_score(labels_test, y_pred_test)\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "\n",
    "# 計算混淆矩陣並繪製圖形\n",
    "cm = confusion_matrix(labels_test, y_pred_test)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted Labels')\n",
    "axes[0].set_ylabel('True Labels')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# 計算精確率、召回率、F1分數並繪製圖形\n",
    "report = classification_report(labels_test, y_pred_test, output_dict=True)\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['precision'] for k in list(report.keys())[:-3]], ax=axes[1])\n",
    "axes[1].set_xlabel('Classes')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision')\n",
    "\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['recall'] for k in list(report.keys())[:-3]], ax=axes[2])\n",
    "axes[2].set_xlabel('Classes')\n",
    "axes[2].set_ylabel('Recall')\n",
    "axes[2].set_title('Recall')\n",
    "\n",
    "sns.barplot(x=list(report.keys())[:-3], y=[report[k]['f1-score'] for k in list(report.keys())[:-3]], ax=axes[3])\n",
    "axes[3].set_xlabel('Classes')\n",
    "axes[3].set_ylabel('F1 Score')\n",
    "axes[3].set_title('F1 Score')\n",
    "\n",
    "# 計算ROC曲線並繪製圖形\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(len(np.unique(labels_test))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(labels_test, y_pred_test, pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "for i in range(len(np.unique(labels_test))):\n",
    "    axes[4].plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "axes[4].plot([0, 1], [0, 1], 'k--')\n",
    "axes[4].set_xlabel('False Positive Rate')\n",
    "axes[4].set_ylabel('True Positive Rate')\n",
    "axes[4].set_title('ROC Curve')\n",
    "axes[4].legend(loc='lower right')\n",
    "\n",
    "# 調整子圖之間的間距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 顯示圖形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df103a",
   "metadata": {},
   "source": [
    "# 非監督式學習 使用 K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 KMeans 聚類模型\n",
    "n_clusters = len(np.unique(labels_data))  # 設置聚類數量為標籤的唯一值數量\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# 訓練模型\n",
    "kmeans.fit(angles_data_2d)\n",
    "\n",
    "# 獲取聚類標籤\n",
    "labels_pred = kmeans.labels_\n",
    "\n",
    "# 評估聚類質量\n",
    "silhouette_avg = silhouette_score(angles_data_2d, labels_pred)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# 可視化聚類結果\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# 繪製散點圖，根據聚類標籤設置顏色\n",
    "unique_labels = np.unique(labels_pred)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = labels_pred == label\n",
    "    ax.scatter(angles_data_2d[mask, 0], angles_data_2d[mask, 1], c=[color], label=f\"Cluster {label}\", alpha=0.8)\n",
    "\n",
    "# 繪製聚類中心\n",
    "centers = kmeans.cluster_centers_\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5, marker='x', label='Cluster Centers')\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('K-means Clustering')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 計算聚類標籤與真實標籤的匹配情況\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# 計算聚類標籤與真實標籤的混淆矩陣\n",
    "cm = np.zeros((n_clusters, n_clusters))\n",
    "for i in range(n_clusters):\n",
    "    for j in range(n_clusters):\n",
    "        cm[i, j] = np.sum((labels_pred == i) & (labels_data == j))\n",
    "\n",
    "# 使用匈牙利算法找到最佳的聚類標籤與真實標籤的匹配\n",
    "row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "labels_pred_matched = np.zeros_like(labels_pred)\n",
    "for i in range(n_clusters):\n",
    "    labels_pred_matched[labels_pred == row_ind[i]] = col_ind[i]\n",
    "\n",
    "# 計算匹配後的準確率\n",
    "accuracy = accuracy_score(labels_data, labels_pred_matched)\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對特徵進行標準化\n",
    "scaler = StandardScaler()\n",
    "angles_data_scaled = scaler.fit_transform(angles_data_2d)\n",
    "\n",
    "# 使用 PCA 進行降維\n",
    "pca = PCA(n_components=0.95)  # 保留 95% 的方差\n",
    "angles_data_pca = pca.fit_transform(angles_data_scaled)\n",
    "\n",
    "# 創建 KMeans 聚類模型\n",
    "n_clusters = len(np.unique(labels_data))  # 設置聚類數量為標籤的唯一值數量\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "# 訓練模型\n",
    "kmeans.fit(angles_data_pca)\n",
    "\n",
    "# 獲取聚類標籤\n",
    "labels_pred = kmeans.labels_\n",
    "\n",
    "# 評估聚類質量\n",
    "silhouette_avg = silhouette_score(angles_data_pca, labels_pred)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "\n",
    "# 可視化聚類結果\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# 繪製散點圖，根據聚類標籤設置顏色\n",
    "unique_labels = np.unique(labels_pred)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = labels_pred == label\n",
    "    ax.scatter(angles_data_pca[mask, 0], angles_data_pca[mask, 1], c=[color], label=f\"Cluster {label}\", alpha=0.8)\n",
    "\n",
    "# 繪製聚類中心\n",
    "centers = kmeans.cluster_centers_\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5, marker='x', label='Cluster Centers')\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('K-means Clustering (with PCA)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 計算聚類標籤與真實標籤的匹配情況\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# 計算聚類標籤與真實標籤的混淆矩陣\n",
    "cm = np.zeros((n_clusters, n_clusters))\n",
    "for i in range(n_clusters):\n",
    "    for j in range(n_clusters):\n",
    "        cm[i, j] = np.sum((labels_pred == i) & (labels_data == j))\n",
    "\n",
    "# 使用匈牙利算法找到最佳的聚類標籤與真實標籤的匹配\n",
    "row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "labels_pred_matched = np.zeros_like(labels_pred)\n",
    "for i in range(n_clusters):\n",
    "    labels_pred_matched[labels_pred == row_ind[i]] = col_ind[i]\n",
    "\n",
    "# 計算匹配後的準確率\n",
    "accuracy = accuracy_score(labels_data, labels_pred_matched)\n",
    "print(f\"Clustering Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24de492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port=12345\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32e043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
